---
title: 关于深度学习的片面总结
date: 2021-07-01 17:52:41
categories: 学习日志
tags:
  - 机器学习
  - 深度学习
  - 神经网络
  - Machine Learning
  - Deep Learning
  - Neural Network
mathjax: true
toc: true
---

## 前言

Deep Learning 一书被成为AI圣经，中文译者张志华老师说道：它告诉我们深度学习集技术、科学与艺术于一体，涉及统计、优化、矩阵、算法、编程、分布式计算等多个邻域。

这也意味着深度学习的门槛，当我作为一个萌新学者看这本书的时候，实在是难读和枯燥，但通过一年的学习再回来看这本书，收获颇丰，所以我决定再次研读“圣经”，并结合相应邻域的论文学习，同时我会尽可能的实践，然后根据拙见逐步的写完这篇总结，毕竟行万里路读万卷书嘛。

<!--more-->

吐槽：慢慢学比较快！

## 机器学习基础

我先总结下深度学习的模型，这里之后再来补充
break

## 结构化概率模型

### 使用图描述模型结构

- 有向模型（信念网络，贝叶斯网络）
  - 高效的从模型中抽取样本
- 无向模型（马尔可夫随机场MRF）
  - 推导近似判断的过程

### 结构化概率模型的深度学习方法

- 图节点之间通过潜变量来表示
- 实例：受限玻尔兹曼机

## 蒙特卡罗方法

### 蒙特卡罗采样近似

- 无法精确计算或积分时
- 重要采样

### 马尔可夫链蒙特卡罗方法（MCMC）

- 马尔可夫链
  - 人生哲理：The future is independent of the past given the present. 未来独立于过去，只基于当下。
  - 转移概率矩阵（状态分布矩阵）
  - 细致平稳条件（前后两个状态可以来回转换）

- MCMC采样
  - 引入接受率alpha形成新的转移矩阵Q（按等式对称性）（这里书上写的是真的难懂😥）$$p(i) \underbrace{q(i, j) \alpha(i, j)}_{Q^{\prime}(i, j)}=p(j) \underbrace{q(j, i) \alpha(j, i)}_{Q^{\prime}(j, i)}$$
  - 从均匀分布采样与转移概率比较判断是否转移（随机过程）
  - M-H(Metropolis-Hastings)采样使等式两边的转移概率比来随机转移，改进了收敛太慢的缺陷

### Gibbs采样

- 重新寻找细致平稳条件
  - 解决计算量与高维特征联合概率不好求问题
  - 将样本放到空间中来看，在同一条线上，条件概率分布作为转移概率满足平稳条件
  - 推广到高维空间同理

