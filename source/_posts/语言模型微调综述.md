---
title: 语言模型微调综述
date: 2023-06-29 10:00:00
categories: 学习日志
tags:
  - 大语言模型
  - 微调
  - 人工智能
  - 神经网络
toc: true
---

## 前言

微调的方法越来越有效，开源环境也越做越好，是真佩服这些大佬。

干啥啥不行，抄代码第一名。有时间稍微看点综述吧，整个脉络和体系应该再了解一下。

参考：[Prompt-Tuning：深度解读一种新的微调范式](https://wjn1996.blog.csdn.net/article/details/120607050)，[让天下没有难Tuning的大模型-PEFT技术简介](https://www.yuque.com/meta95/hmc3l4/ozgy13dx4akv7v17?singleDoc#)

<!--more-->

## Fine-tuning

### 预训练模型
    
- Masked Language Modeling（MLM）

    遮掩式的自监督训练

    一些变体：

    - Whole Word Masking（WWM）分词处理

    - Entity Mention Replacement（EMR）实体遮掩

- Next Sentence Prediction（NSP）

    句子的关系处理，通过```[cls]```来做判断

    作用不明显，大部分任务不做NSP

### 下游任务

容易过拟合，难以持续学习

## Prompt-Tuning

- 定义

    - 构建模板（Template Construction）

    - 标签词映射（Label Word Verbalizer）

- GPT-3和PET

    Prompt-Tuning的鼻祖，定义中的两个组件

- Prompt是对下游任务的指令，是一种信息增强

- Prompt是参数有效性学习

- 基于Prompt的统一范式

    - 万物皆可生成

    - 万物皆可抽取

    - 万物皆可推理

- 大模型的Prompt-Tuning

    - 上下文学习 In-Context Learning（ICL）

    - 指令学习 Instruction-tuning 
    
    - 思维链 Chain-of-Thought（CoT）

## P-Tuning

- 比Prompt-Tuning更优雅的存在

    P-Tuning的prompt是token，是soft prompt、是隐式的、经过训练的、模型认为最优的prompt token。

- P-Tuning v2甚至在每层增加token，[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)的ptuning效果很好。

## LoRA

- Low-Rank Adaptation

    CV NLP 两开花

## 训练的一些tips

- 对抗式的学习（FGM, PGD，FreeLB）

- 指数移动平均（EMA）

