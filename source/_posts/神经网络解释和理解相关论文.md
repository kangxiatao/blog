---
title: 神经网络解释和理解相关论文
date: 2021-09-23 15:32:32
categories: 学习日志
tags:
  - 神经网络
mathjax: true
toc: true
---

## ==>==> 快进到2077 ==>==>

可解释性 —— 人工智能过不去的一道坎
志大才疏，待续，

<!--more-->

## 预训练

从剪枝做到可解释性预训练，只能说望尘莫及。

预训练模型

- 图像领域：ImageNet训练的模型迁移到下游任务
- NLP领域：包含上下文信息的BERT预训练模型

预训练的应用（特征迁移和参数迁移）

- 特征提取。去掉预训练模型的输出层，做为固定的特征提取机。
- 冻结特定层，只训练后面的层。ALBERT: https://arxiv.org/abs/1909.11942

多模态融合

- ViLBERT: https://arxiv.org/abs/1908.02265
- ImageBERT: https://arxiv.org/abs/2001.07966

预训练的解释

- 两个假设来解释预训练的效果：（1）更好的优化（2）更好的正则化。

  (2010)[Why Does Unsupervised Pre-training Help Deep Learning?]([https://](http://proceedings.mlr.press/v9/erhan10a.html))
  
- 引入了潜在类的概念，弥合预训练和微调之间的差距。（迁移到下游时可以有更低的loss）

  (2019)[A Theoretical Analysis of Contrastive Unsupervised Representation Learning]([https://](https://arxiv.org/abs/1902.09229))

## 泛化

### Stronger generalization bounds for deep nets via a compression approach

通过压缩方法使深度神经网络获得更强泛化能力

作者为压缩网络提升泛化提供了一些理论依据，提出一个证明泛化边界的简单压缩框架，提出识别网络中噪声的方法。

### Generalization and Expressivity for Deep Nets

深度神经网络的泛化和表达

作者发现深网具有良好的表达能力(通过局部和稀疏逼近来衡量)，而没有本质上扩大浅网的容量。基于这个结论，作者得出经验风险最小化(ERM)的最优学习率。

## #、---
