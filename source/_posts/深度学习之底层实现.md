---
title: 深度学习之底层实现
date: 2022-09-29 11:01:43
categories: 学习日志
tags:
  - 深度学习
  - 深度框架
  - CUDA编程
mathjax: true
toc: true
---

## 前言

得益于AI的发展，更感谢这几年前辈们的努力，我们拥有一个很友好的深度学习开发环境。但做为一个即将步入工业界的码农来说，不懂底层是很残酷的一件事。幸运的是人生不一样，明白了国家机器的运作，搞懂了金融体系的构成，甚至看清了生活的本质，然后还热爱着并饱含初心，确实是真正的英雄。

<!--more-->

## Tensor的底层原理

张量概念在物理学中是矢量概念的推广，在计算机中的表现形式其实就是一个多维数组。

而在计算机的存储中都是以一维存储的，然后用循环或动态规划的方法转换成高维。比如在Pytorch中，Tensor通过Storage进行封装，由Size、storage offset和strides三个属性来组成多维数组。

## 计算图

在深度学习一书中经常用计算图来描述算法过程，实际上神经网络框架就是利用计算图来提高效率的。计算图使得计算过程清晰简洁，也有利于反向传播。

计算图根据搭建方式分为动态图和静态图，Pytorch采用动态图机制，Tensorflow1.0版本采用静态图机制，Tensorflow2.0改用了动态图机制。

动态图：边搭建图边计算，具有灵活且易调节的优点；静态图：先搭建图，后运算，高效但不灵活。

## 框架结构

一个完整的深度学习框架主要由数据载体模块、数据存储模块、神经网络模块、求导模块、优化器模块、加速模块和效率工具模块组成。

对于求导，本质上是向量-雅克比乘积(vector-Jacobian product)，雅克比矩阵通常用迭代法求数值解。后续如果工作需要，部分模块的底层算子慢慢在这里补充。此处省略一万字。

## 

