---
title: 次优示例强化学习实验总结
date: 2024-01-19 12:00:00
categories: 学习日志
tags:
  - 强化学习
  - 人工智能
  - Reinforcement Learning
  - Neural Network
toc: true
---

## 前言

跟着科大老师搞了一段时间的强化学习研究，论文投出去我这边也算是可以收尾了，不管后面怎么样，还是有不少收获和成就感的。论文方面实验还是很多瑕疵，有些图表也没来得及做，有点可惜。总之，这段时间很有意思，也没有很累，倒还得了不少称赞。

也许但行好事莫问前程，或是东风吹送好运来。在校期间似乎从来没争取过什么，自己凭兴趣随便散散步，有认真搞学习，倒也谈不上很努力。傻傻的自释觉悟者，工作都没有认真找，主打一个无所谓。以至于我总是不理解为什么很多事情像安排好一样，等着我走过去给我一个reward，可我真的只是在散步。钝鸟随便飞，大器无须成。

sorry，跑题了。回到主题，研究设定是在极端稀疏且奖励滞后的环境下通过次优示例学习到最优策略。下面做了一个简单的总结，希望后面还有深入研究的机会吧。

<!--more-->

## 环境

环境改造要注意完成和不完成任务两种轨迹的差异，一方面完成任务应该得到更大的奖励，另一方面相似的轨迹奖励分布要相似，否则训练很难收敛。

## 终态

终态done的设定影响很大。

通常观测量没有加入时间状态时，截断伪终态会处理为false。在吸收态环境中，加入时间维度之后，把截断当超时处理并标记状态为true训练要更稳定，性能也更好一点。

但是在猎豹类的环境中，不存在超时，摔倒的终态很重要，不可随意处理。另外，若在摔倒状态下做奖励惩罚能取得更好的性能。

## 轨迹质量

轨迹质量的评估相对模糊，也很少有文章讨论。

在mujoco的几个环境中，控制奖励的存在使得单独以周期奖励评估轨迹质量不合理。比如reacher上在每一步上加一点点抖动，但可以到目标点，此时奖励为-15，而在原地不动可能有-10的奖励。此时-15的示例很容易训练出来，而-10的示例就有一定挑战。

这也是我们敏感性分析中质量和数量的性能不一定线性变化的原因。

## 奖励重标记

目前重标记最大的问题就是容易导致相似的状态-动作组对应不同奖励，影响模型收敛。另外我们的重标记也不能标记出关键的状态-动作组，如在更复杂的环境下不一定有效。在half类环境性能还不错得益于这类环境轨迹中状态的高度重复。

## buffer

在较大的size下，再调整buffer大小对性能影响不大。

专家buffer的比例是较大的影响因素。在BC的约束下，加大专家buffer比例能有更好的性能，预训练后写满专家buffer也有一定的提升。

## BC

BC在不同环境下表现出的效果完全不一样，和示例demo质量也有一定关系。

在reacher和pusher上，仅bc自模仿就能学出还不错的策略，pusher强依赖bc。

而在half和hopper上，预训练后，不需要bc也能取得不错的性能，尤其在加了失败惩罚之后，仅重标记和rl甚至有更好的性能。

## 衰减

学习率衰减并不能解决训练后期大幅度波动问题，只能缓解，仍会出现性能急速下降，且回升更慢。

BC和采样衰减的初衷是为增加探索奖励，在没探索奖励的情况下基本上没有正作用。

## 裁剪

裁剪能解决训练过程的波动，尤其pusher这种重标记后相似轨迹差异较大的情况。但是裁剪也会限制训练，一方面会减缓训练速度，另一方面减小了不同实验参数之间的差异，会影响消融实验。

## 探索

探索的随机性相当大，超参数调节也比较繁琐，探索的奖励控制在重标记后奖励幅度的10%较合理。

目前测试的两种探索都没有很明显的效果，不过在参数合适的情况下会减少大幅度波动的情况，探索奖励过大产生的副作用也比较大。
