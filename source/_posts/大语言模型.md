---
title: 大语言模型
date: 2023-04-20 19:11:02
categories: 学习日志
tags:
  - 人工智能
  - 神经网络
  - 大语言模型
toc: true
---

## AGI的火花

一篇很好的综述：[大型语言模型（LLM）技术精要](https://zhuanlan.zhihu.com/p/597586623)

[大语言模型调研汇总](https://zhuanlan.zhihu.com/p/614766286)

<!--more-->

### [GPT](https://arxiv.org/abs/2303.08774)

pass

### [LaMDA](https://arxiv.org/abs/2201.08239)

pass

### [PaLM](https://arxiv.org/abs/2204.02311)

pass

### [GLM-130B](https://arxiv.org/abs/2204.02311)

清华中英双语语言模型，在量化方面有优势，开源。

### [LLaMA](https://arxiv.org/abs/2302.13971v1)
    
Meta AI推出了大语言模型LLaMA，包括7B、13B、33B和65B几个版本，与GPT3性能相当。

10G显存可跑：8-bit LLaMA 7B，4-bit LLaMA 7B，4-bit LLaMA 13B 。

测试了7B模型，效果一般，需要针对性微调。

- 羊驼家族

    LLaMA的模型泄漏诞生了羊驼家族：Alpaca、Vicuna、Koala、ChatLLaMA 、FreedomGPT、ColossalChat...，基本上都是指令微调的结果。

    - [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
        
        由LLaMA 7B微调而来，仅用52k数据，训练成本低

        - [Alpaca-LoRA](https://github.com/tloen/alpaca-lora)

            LoRA微调
        
        - Alpaca-ChToken and [Luotuo](https://github.com/LC1332/Luotuo-Chinese-LLM)

            中文token，Luotuo的开源做得很好

    - Vicuna

        Vicuna-13B接近Bard

    - [FreedomGPT](https://github.com/ohmplatform/FreedomGPT)

        输出不受限制的AI模型，Freedom!
        
## 大模型的研究热点

LLaMA的轻量级版本和微软开源的[Deep Speed Chat](https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md)很大程度降低了大语言模型的门槛，相关的研究和应用已是呈爆炸式增长。

- 大模型的微调

    - 提示学习（prompt learning）

    - [指令学习](https://arxiv.org/abs/2109.01652)（instruction learning）

    - [LoRA](https://arxiv.org/abs/2106.09685)微调（[Alpaca-LoRA](https://github.com/tloen/alpaca-lora)）

- 大模型的压缩和加速

    - [SparseGPT](https://arxiv.org/abs/2301.00774)

    - [Compression for AGI](https://www.nxrte.com/jishu/16893.html)

- 大模型的能力涌现和多模态

    不存在了，可解释性不存在了!

    - [Kosmos-1](https://arxiv.org/abs/2302.14045)

    - [PALM-E](https://palm-e.github.io/)

    - [Visual ChatGPT](https://arxiv.org/abs/2303.04671)

- LLM = Compression

    OpenAI的新观点：大语言模型的本质，其实是一个性能强大的数据无损压缩器。

- ...

## 个人想法

目前大模型的能力及其惊人，模型轻量化必然是现有阶段的关键环节。个人感觉大模型蒸馏和剪枝的方法即将遍地开花（前几年在预训练模型上的蒸馏就很多），虽然大模型的训练成本依旧很高，但做大模型压缩的研究绝对是有利可图的热点领域。

- 下游任务微调过程中的修剪

    依据模型对数据的敏感修剪网络，应用到特定小场景。如：将LLaMA大模型修剪为只写投标文件的小模型（指令学习+剪枝）。

    参考一些[针对Transformer模型的修剪](https://arxiv.org/abs/2206.12562)

- 基于预训练模型的修剪

    类似于训练前修剪的研究，甚至修剪后不做训练，也可以针对特定场景。

- LoRA的变体

    LoRA实在是太有效了，LoRA和剪枝的结合

- 利用大模型对轻量模型的蒸馏设计

    如用chatgpt做评估训练一个轻量的问答系统

- 顺着大模型的思路去，设计大模型来修剪大模型，修剪一切！

    瞎扯一下，大模型可以Segment Anything，大模型也可以Pruning Anything！
